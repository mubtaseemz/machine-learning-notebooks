{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Set shell to show all lines of output\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37020 books.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "books = []\n",
    "\n",
    "with open('found_books_filtered.ndjson', 'r') as fin:\n",
    "    # Append each line to the books\n",
    "    books = [json.loads(l) for l in fin]\n",
    "\n",
    "# Remove non-book articles\n",
    "books_with_wikipedia = [book for book in books if 'Wikipedia:' in book[0]]\n",
    "books = [book for book in books if 'Wikipedia:' not in book[0]]\n",
    "print(f'Found {len(books)} books.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia:Wikipedia Signpost/2014-06-25/Recent research',\n",
       " 'Wikipedia:New pages patrol/Unpatrolled articles/December 2010',\n",
       " 'Wikipedia:Templates for discussion/Log/2012 September 23',\n",
       " 'Wikipedia:Articles for creation/Redirects/2012-10',\n",
       " 'Wikipedia:Templates for discussion/Log/2012 October 4']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[book[0] for book in books_with_wikipedia][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Limonov (novel)',\n",
       " {'name': 'Limonov',\n",
       "  'author': 'Emmanuel Carrère',\n",
       "  'translator': 'John Lambert',\n",
       "  'country': 'France',\n",
       "  'language': 'French',\n",
       "  'publisher': 'P.O.L.',\n",
       "  'pub_date': '2011',\n",
       "  'english_pub_date': '2014',\n",
       "  'pages': '488',\n",
       "  'isbn': '978-2-8180-1405-9'},\n",
       " ['Emmanuel Carrère',\n",
       "  'biographical novel',\n",
       "  'Emmanuel Carrère',\n",
       "  'Eduard Limonov',\n",
       "  'Prix de la langue française'],\n",
       " ['http://www.lefigaro.fr/flash-actu/2011/10/05/97001-20111005FILWWW00615-le-prix-de-la-langue-francaise-a-e-carrere.php',\n",
       "  'http://www.lexpress.fr/culture/livre/emmanuel-carrere-prix-renaudot-2011_1046819.html',\n",
       "  'http://limonow.de/carrere/index.html',\n",
       "  'http://www.tout-sur-limonov.fr/222318809'],\n",
       " '2018-08-18T02:03:21Z',\n",
       " 1437)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 21\n",
    "books[n][0], books[n][1], books[n][2][:5], books[n][3][:5], books[n][4], books[n][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22494"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Anna Karenina'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_index = {book[0]: idx for idx, book in enumerate(books)}\n",
    "index_book = {idx: book for book, idx in book_index.items()}\n",
    "\n",
    "book_index['Anna Karenina']\n",
    "index_book[22494]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 311276 unique wikilinks.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "wikilinks = list(chain(*[book[2] for book in books]))\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17032 unique wikilinks to other books.\n"
     ]
    }
   ],
   "source": [
    "wikilinks_other_books = [link for link in wikilinks if link in book_index.keys()]\n",
    "print(f\"There are {len(set(wikilinks_other_books))} unique wikilinks to other books.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def count_items(l):\n",
    "    \"\"\"Return ordered dictionary of counts of objects in `l`\"\"\"\n",
    "    \n",
    "    # Create a counter object\n",
    "    counts = Counter(l)\n",
    "    \n",
    "    # Sort by highest count first and place in ordered dictionary\n",
    "    counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)\n",
    "    counts = OrderedDict(counts)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hardcover', 7489),\n",
       " ('Paperback', 7311),\n",
       " ('Wikipedia:WikiProject Books', 6043),\n",
       " ('Wikipedia:WikiProject Novels', 6015),\n",
       " ('English language', 4185),\n",
       " ('United States', 3060),\n",
       " ('Science fiction', 3030),\n",
       " ('The New York Times', 2727),\n",
       " ('science fiction', 2502),\n",
       " ('novel', 1979)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of wikilinks for each book and convert to a flattened list\n",
    "unique_wikilinks = list(chain(*[list(set(book[2])) for book in books]))\n",
    "\n",
    "wikilink_counts = count_items(unique_wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 297624 unique wikilinks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('paperback', 8740),\n",
       " ('hardcover', 8648),\n",
       " ('wikipedia:wikiproject books', 6043),\n",
       " ('wikipedia:wikiproject novels', 6016),\n",
       " ('science fiction', 5665),\n",
       " ('english language', 4248),\n",
       " ('united states', 3063),\n",
       " ('novel', 2983),\n",
       " ('the new york times', 2742),\n",
       " ('fantasy', 2003)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilinks = [link.lower() for link in unique_wikilinks]\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")\n",
    "\n",
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['hardcover', 'paperback', 'hardback', 'e-book', 'wikipedia:wikiproject books', 'wikipedia:wikiproject novels']\n",
    "for t in to_remove:\n",
    "    wikilinks.remove(t)\n",
    "    _ = wikilink_counts.pop(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41758\n"
     ]
    }
   ],
   "source": [
    "# Limit to greater than 3 links\n",
    "links = [t[0] for t in wikilink_counts.items() if t[1] >= 4]\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Encyclopedia of Science Fiction', 127),\n",
       " ('The Discontinuity Guide', 104),\n",
       " ('The Encyclopedia of Fantasy', 63),\n",
       " ('Dracula', 55),\n",
       " ('Encyclopædia Britannica', 51),\n",
       " ('Nineteen Eighty-Four', 51),\n",
       " ('Don Quixote', 49),\n",
       " ('The Wonderful Wizard of Oz', 49),\n",
       " (\"Alice's Adventures in Wonderland\", 47),\n",
       " ('Jane Eyre', 39)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of book wikilinks for each book\n",
    "unique_wikilinks_books = list(chain(*[list(set(link for link in book[2] if link in book_index.keys())) for book in books]))\n",
    "\n",
    "# Count the number of books linked to by other books\n",
    "wikilink_book_counts = count_items(unique_wikilinks_books)\n",
    "list(wikilink_book_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'the economist'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41758 wikilinks that will be used.\n"
     ]
    }
   ],
   "source": [
    "link_index = {link: idx for idx, link in enumerate(links)}\n",
    "index_link = {idx: link for link, idx in link_index.items()}\n",
    "\n",
    "link_index['the economist']\n",
    "index_link[300]\n",
    "print(f'There are {len(link_index)} wikilinks that will be used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(772798, 41758, 37020)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(321, 232)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "\n",
    "# Iterate through each book\n",
    "for book in books:\n",
    "    # Iterate through the links in the book\n",
    "    pairs.extend((book_index[book[0]], link_index[link.lower()]) for link in book[2] if link.lower() in links)\n",
    "    \n",
    "len(pairs), len(links), len(books)\n",
    "pairs[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Slaves in the Family', 'category:american biographies')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[pairs[5000][0]], index_link[pairs[5000][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Man Who Watched the Trains Go By (novel)',\n",
       " 'category:belgian novels adapted into films')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[pairs[900][0]], index_link[pairs[900][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_set = set(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((13337, 31112), 85),\n",
       " ((31899, 65), 77),\n",
       " ((25899, 8851), 61),\n",
       " ((1851, 2629), 57),\n",
       " ((25899, 30465), 53)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Counter(pairs)\n",
    "sorted(x.items(), key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"France's Songs of the Bards of the Tyne - 1850\", 'robert nunn (songwriter)')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('The Early Stories: 1953–1975', 'the new yorker')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('Marthandavarma (novel)', 'kerala sahitya akademi')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[13337], index_link[31111]\n",
    "index_book[31899], index_link[65]\n",
    "index_book[25899], index_link[30465]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0, classification = False):\n",
    "    \"\"\"Generate batches of samples for training\"\"\"\n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    \n",
    "    # Adjust label based on task\n",
    "    if classification:\n",
    "        neg_label = 0\n",
    "    else:\n",
    "        neg_label = -1\n",
    "    \n",
    "    # This creates a generator\n",
    "    while True:\n",
    "        # randomly choose positive examples\n",
    "        for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (book_id, link_id, 1)\n",
    "\n",
    "        # Increment idx by 1\n",
    "        idx += 1\n",
    "        \n",
    "        # Add negative examples until reach batch size\n",
    "        while idx < batch_size:\n",
    "            \n",
    "            # random selection\n",
    "            random_book = random.randrange(len(books))\n",
    "            random_link = random.randrange(len(links))\n",
    "            \n",
    "            # Check to make sure this is not a positive example\n",
    "            if (random_book, random_link) not in pairs_set:\n",
    "                \n",
    "                # Add to batch and increment index\n",
    "                batch[idx, :] = (random_book, random_link, neg_label)\n",
    "                idx += 1\n",
    "                \n",
    "        # Make sure to shuffle order\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2] # yield is needed for generators. Helps in memory management\n",
    "        # can call with next() or with a for loop for i in infinite_sequence():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'book': array([29880.,  4031.,  6018.,  3138., 20277., 21977.]),\n",
       "  'link': array([17263.,   202.,  2826., 13401., 13360., 15053.])},\n",
       " array([-1.,  1.,  1., -1., -1., -1.]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: Emergence (novel)              Link: royal military academy sandhurst         Label: -1.0\n",
      "Book: The Knife and the Butterfly    Link: nbc news                                 Label: 1.0\n",
      "Book: Brave New Girl (novel)         Link: heidi pitlor                             Label: -1.0\n",
      "Book: Ug (book)                      Link: elayne trakand                           Label: -1.0\n",
      "Book: I, Coriander                   Link: children's literature                    Label: 1.0\n",
      "Book: Cousin Kate                    Link: category:novels by ken follett           Label: -1.0\n"
     ]
    }
   ],
   "source": [
    "x, y = next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))\n",
    "\n",
    "# Show a few example training pairs\n",
    "for label, b_idx, l_idx in zip(y, x['book'], x['link']):\n",
    "    print(f'Book: {index_book[b_idx]:30} Link: {index_link[l_idx]:40} Label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Dot, Reshape, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mubtaseem Zaman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "book (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "link (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "book_embedding (Embedding)      (None, 1, 50)        1851000     book[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "link_embedding (Embedding)      (None, 1, 50)        2087900     link[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dot_product (Dot)               (None, 1, 1)         0           book_embedding[0][0]             \n",
      "                                                                 link_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1)            0           dot_product[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 3,938,900\n",
      "Trainable params: 3,938,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def book_embedding_model(embedding_size = 50, classification = False):\n",
    "    \"\"\"Model to embed books and wikilinks using the functional API.\n",
    "       Trained to discern if a link is present in a article\"\"\"\n",
    "    \n",
    "    # Both inputs are 1-dimensional\n",
    "    book = Input(name = 'book', shape = [1])\n",
    "    link = Input(name = 'link', shape = [1])\n",
    "    \n",
    "    # Embedding the book (shape will be (None, 1, 50))\n",
    "    book_embedding = Embedding(name = 'book_embedding',\n",
    "                               input_dim = len(book_index),\n",
    "                               output_dim = embedding_size)(book)\n",
    "    \n",
    "    # Embedding the link (shape will be (None, 1, 50))\n",
    "    link_embedding = Embedding(name = 'link_embedding',\n",
    "                               input_dim = len(link_index),\n",
    "                               output_dim = embedding_size)(link)\n",
    "    \n",
    "    # Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))\n",
    "    merged = Dot(name = 'dot_product', normalize = True, axes = 2)([book_embedding, link_embedding])\n",
    "    \n",
    "    # Reshape to be a single number (shape will be (None, 1))\n",
    "    merged = Reshape(target_shape = [1])(merged)\n",
    "    \n",
    "    # If classifcation, add extra layer and loss function is binary cross entropy\n",
    "    if classification:\n",
    "        merged = Dense(1, activation = 'sigmoid')(merged)\n",
    "        model = Model(inputs = [book, link], outputs = merged)\n",
    "        model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # Otherwise loss function is mean squared error\n",
    "    else:\n",
    "        model = Model(inputs = [book, link], outputs = merged)\n",
    "        model.compile(optimizer = 'Adam', loss = 'mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate model and show parameters\n",
    "model = book_embedding_model() #this function initializes and returns the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mubtaseem Zaman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      " - 45s - loss: 1.0142\n",
      "Epoch 2/15\n",
      " - 44s - loss: 1.0051\n",
      "Epoch 3/15\n",
      " - 45s - loss: 0.9908\n",
      "Epoch 4/15\n",
      " - 45s - loss: 0.9693\n",
      "Epoch 5/15\n",
      " - 43s - loss: 0.9529\n",
      "Epoch 6/15\n",
      " - 44s - loss: 0.9296\n",
      "Epoch 7/15\n",
      " - 51s - loss: 0.8369\n",
      "Epoch 8/15\n",
      " - 50s - loss: 0.8664\n",
      "Epoch 9/15\n",
      " - 48s - loss: 0.8228\n",
      "Epoch 10/15\n",
      " - 49s - loss: 0.8330\n",
      "Epoch 11/15\n",
      " - 48s - loss: 0.8353\n",
      "Epoch 12/15\n",
      " - 47s - loss: 0.8456\n",
      "Epoch 13/15\n",
      " - 51s - loss: 0.8196\n",
      "Epoch 14/15\n",
      " - 51s - loss: 0.7989\n",
      "Epoch 15/15\n",
      " - 44s - loss: 0.8713\n"
     ]
    }
   ],
   "source": [
    "n_positive = 1024\n",
    "\n",
    "gen = generate_batch(pairs, n_positive, negative_ratio = 2)\n",
    "\n",
    "# Train\n",
    "h = model.fit_generator(gen, epochs = 15, \n",
    "                        steps_per_epoch = len(pairs) // n_positive,\n",
    "                        verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/first_attempt.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37020, 50)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract embeddings\n",
    "book_layer = model.get_layer('book_embedding')\n",
    "book_weights = book_layer.get_weights()[0]\n",
    "book_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03387945,  0.06018053, -0.0937172 , -0.06575911,  0.12377819,\n",
       "        0.23197177, -0.05466267,  0.09391195,  0.04234951,  0.05262422],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_weights[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04665903,  0.08288107, -0.12906802, -0.09056395,  0.17046826,\n",
       "        0.31947327, -0.07528184,  0.12933624,  0.05832406,  0.07247447],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.99999994"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_weights = book_weights / np.linalg.norm(book_weights, axis = 1).reshape((-1, 1)) # root(sum squares of row elements) -> make a colum vector\n",
    "book_weights[0][:10]\n",
    "np.sum(np.square(book_weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 15\n",
    "\n",
    "def find_similar(name, weights, index_name = 'book', n = 10, least = False, return_dist = False, plot = False):\n",
    "    \"\"\"Find n most similar items (or least) to name based on embeddings. Option to also plot the results\"\"\"\n",
    "    \n",
    "    # Select index and reverse index\n",
    "    if index_name == 'book':\n",
    "        index = book_index\n",
    "        rindex = index_book\n",
    "    elif index_name == 'page':\n",
    "        index = link_index\n",
    "        rindex = index_link\n",
    "    \n",
    "    # Check to make sure `name` is in index\n",
    "    try:\n",
    "        # Calculate dot product between book and all others\n",
    "        dists = np.dot(weights, weights[index[name]])\n",
    "    except KeyError:\n",
    "        print(f'{name} Not Found.')\n",
    "        return\n",
    "    \n",
    "    # Sort distance indexes from smallest to largest\n",
    "    sorted_dists = np.argsort(dists)\n",
    "    \n",
    "    # Plot results if specified\n",
    "    if plot:\n",
    "        \n",
    "        # Find furthest and closest items\n",
    "        furthest = sorted_dists[:(n // 2)]\n",
    "        closest = sorted_dists[-n-1: len(dists) - 1]\n",
    "        items = [rindex[c] for c in furthest]\n",
    "        items.extend(rindex[c] for c in closest)\n",
    "        \n",
    "        # Find furthest and closets distances\n",
    "        distances = [dists[c] for c in furthest]\n",
    "        distances.extend(dists[c] for c in closest)\n",
    "        \n",
    "        colors = ['r' for _ in range(n //2)]\n",
    "        colors.extend('g' for _ in range(n))\n",
    "        \n",
    "        data = pd.DataFrame({'distance': distances}, index = items)\n",
    "        \n",
    "        # Horizontal bar chart\n",
    "        data['distance'].plot.barh(color = colors, figsize = (10, 8),\n",
    "                                   edgecolor = 'k', linewidth = 2)\n",
    "        plt.xlabel('Cosine Similarity');\n",
    "        plt.axvline(x = 0, color = 'k');\n",
    "        \n",
    "        # Formatting for italicized title\n",
    "        name_str = f'{index_name.capitalize()}s Most and Least Similar to'\n",
    "        for word in name.split():\n",
    "            # Title uses latex for italize\n",
    "            name_str += ' $\\it{' + word + '}$'\n",
    "        plt.title(name_str, x = 0.2, size = 28, y = 1.05)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # If specified, find the least similar\n",
    "    if least:\n",
    "        # Take the first n from sorted distances\n",
    "        closest = sorted_dists[:n]\n",
    "         \n",
    "        print(f'{index_name.capitalize()}s furthest from {name}.\\n')\n",
    "        \n",
    "    # Otherwise find the most similar\n",
    "    else:\n",
    "        # Take the last n sorted distances\n",
    "        closest = sorted_dists[-n:]\n",
    "        \n",
    "        # Need distances later on\n",
    "        if return_dist:\n",
    "            return dists, closest\n",
    "        \n",
    "        \n",
    "        print(f'{index_name.capitalize()}s closest to {name}.\\n')\n",
    "        \n",
    "    # Need distances later on\n",
    "    if return_dist:\n",
    "        return dists, closest\n",
    "    \n",
    "    \n",
    "    # Print formatting\n",
    "    max_width = max([len(rindex[c]) for c in closest])\n",
    "    \n",
    "    # Print the most similar and distances\n",
    "    for c in reversed(closest):\n",
    "        print(f'{index_name.capitalize()}: {rindex[c]:{max_width + 2}} Similarity: {dists[c]:.{2}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books closest to My Real Children.\n",
      "\n",
      "Book: My Real Children                                          Similarity: 1.0\n",
      "Book: Justin Bieber: Just Getting Started                       Similarity: 0.74\n",
      "Book: Federalist No. 24                                         Similarity: 0.74\n",
      "Book: The Culture of Connectivity                               Similarity: 0.71\n",
      "Book: The Sea Hunters: True Adventures with Famous Shipwrecks   Similarity: 0.71\n",
      "Book: The Broker                                                Similarity: 0.71\n",
      "Book: Britain's Road to Socialism                               Similarity: 0.71\n",
      "Book: Federalist No. 16                                         Similarity: 0.71\n",
      "Book: Medusa (Cussler novel)                                    Similarity: 0.7\n",
      "Book: Cathedral of the Sea                                      Similarity: 0.7\n"
     ]
    }
   ],
   "source": [
    "find_similar('My Real Children', book_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
